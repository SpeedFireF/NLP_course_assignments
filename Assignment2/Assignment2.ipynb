{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIgM6C9HYUhm"
   },
   "source": [
    "# Context-sensitive Spelling Correction\n",
    "\n",
    "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
    "\n",
    "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
    "\n",
    "Useful links:\n",
    "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
    "- [Norvig's dataset](https://norvig.com/big.txt)\n",
    "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
    "\n",
    "Grading:\n",
    "- 60 points - Implement spelling correction\n",
    "- 20 points - Justify your decisions\n",
    "- 20 points - Evaluate on a test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-vb8yFOGRDF"
   },
   "source": [
    "## Implement context-sensitive spelling correction\n",
    "\n",
    "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
    "\n",
    "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
    "\n",
    "You may also want to implement:\n",
    "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
    "- some recent (or not very recent) paper on this topic,\n",
    "- solution which takes into account keyboard layout and associated misspellings,\n",
    "- efficiency improvement to make the solution faster,\n",
    "- any other idea of yours to improve the Norvigâ€™s solution.\n",
    "\n",
    "IMPORTANT:  \n",
    "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
    "- Your implementation\n",
    "- Analysis of why the implemented approach is suggested\n",
    "- Improvements of the original approach that you have chosen to implement"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import argparse\n",
    "from itertools import product\n",
    "import math\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import string"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T16:37:28.573586Z",
     "start_time": "2024-03-20T16:37:28.144306Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def combine_files(input_folder, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as combined_file:\n",
    "        folders = ['unsup', 'pos', 'neg']\n",
    "        for folder in folders:\n",
    "            folder_path = os.path.join(input_folder, folder)\n",
    "            files = glob.glob(os.path.join(folder_path, '*.txt'))\n",
    "\n",
    "            for file in files:\n",
    "                with open(file, 'r', encoding='utf-8') as current_file:\n",
    "                    content = current_file.read()\n",
    "                    sentences = [remove_punctuation(sentence.strip()) for sentence in content.split('.') if sentence.strip()]\n",
    "                    combined_file.write('\\n'.join(sentences) + '\\n')\n",
    " \n",
    "input_folder = '/Users/damirabdulaev/Downloads/aclImdb/train'\n",
    "output_file = '/Users/damirabdulaev/Downloads/aclImdb/train/combined.txt'\n",
    "\n",
    "combine_files(input_folder, output_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T16:37:47.175968Z",
     "start_time": "2024-03-20T16:37:28.581001Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_train_data(train_path, percent=50):\n",
    "    with open(train_path, 'r') as f:\n",
    "        train = [l.strip() for l in f.readlines()]\n",
    "    total_samples = len(train)\n",
    "    num_samples_to_load = int(total_samples * percent / 100)\n",
    "    return train[:num_samples_to_load]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T16:37:47.178089Z",
     "start_time": "2024-03-20T16:37:47.176098Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_file = '/Users/damirabdulaev/Downloads/aclImdb/train/combined.txt'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T16:37:47.187396Z",
     "start_time": "2024-03-20T16:37:47.178774Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train = load_train_data(train_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T16:37:47.530233Z",
     "start_time": "2024-03-20T16:37:47.182471Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "SOS = '<s> '\n",
    "EOS = '</s> '\n",
    "UNK = '<UNK>'\n",
    "\n",
    "def end_and_start_tokens(sentences, n):\n",
    "    if n > 1:\n",
    "        sos = SOS * (n-1)\n",
    "    else:\n",
    "        sos = SOS\n",
    "\n",
    "    return [f'{sos}{s} {EOS}' for s in sentences]\n",
    "\n",
    "def replace(tokens):\n",
    "    vocab = nltk.FreqDist(tokens)\n",
    "    return [token if vocab[token] >= 1 else UNK for token in tokens]\n",
    "\n",
    "def preprocess(sentences, n):\n",
    "    sentences = end_and_start_tokens(sentences, n)\n",
    "    tokens = ' '.join(sentences).split(' ')\n",
    "    tokens = replace(tokens)\n",
    "    return tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T16:37:47.539910Z",
     "start_time": "2024-03-20T16:37:47.532806Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<s>', '<s>', '<s>', 'A', 'newspaperman', 'Johnny', 'Twennies', 'living', 'in', 'the', '90s', 'with', 'a', 'complete', '20s', 'personality', 'and', 'lifestyle', '', 'fedora', 'manual', 'typewriter', 'the', 'Charleston', 'the', 'works', '</s>', '', '<s>', '<s>', '<s>', '<s>', 'Its', 'a', 'great', 'idea', 'for', 'a', 'movie', 'and', 'it', 'couldnt', 'have', 'been', 'done', 'better', '</s>', '']\n"
     ]
    }
   ],
   "source": [
    "print(preprocess(train[:2], 5))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T16:37:47.542191Z",
     "start_time": "2024-03-20T16:37:47.538617Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LanguageModel(object):\n",
    "    def __init__(self, train_data, n, laplace=1):\n",
    "        self.n = n\n",
    "        self.laplace = laplace\n",
    "        self.tokens = preprocess(train_data, n)\n",
    "        self.vocab  = nltk.FreqDist(self.tokens)\n",
    "        self.model  = self._create_model()\n",
    "\n",
    "    def _smooth(self):\n",
    "        vocab_size = len(self.vocab)\n",
    "\n",
    "        n_grams = nltk.ngrams(self.tokens, self.n)\n",
    "        n_vocab = nltk.FreqDist(n_grams)\n",
    "\n",
    "        m_grams = nltk.ngrams(self.tokens, self.n-1)\n",
    "        m_vocab = nltk.FreqDist(m_grams)\n",
    "\n",
    "        def smoothed_count(n_gram, n_count):\n",
    "            m_gram = n_gram[:-1]\n",
    "            m_count = m_vocab[m_gram]\n",
    "            return (n_count + self.laplace) / (m_count + self.laplace * vocab_size)\n",
    "\n",
    "        return { n_gram: smoothed_count(n_gram, count) for n_gram, count in n_vocab.items() }\n",
    "    \n",
    "    def _create_model(self):\n",
    "        if self.n == 1:\n",
    "            num_tokens = len(self.tokens)\n",
    "            return { (unigram,): count / num_tokens for unigram, count in self.vocab.items() }\n",
    "        else:\n",
    "            return self._smooth()\n",
    "\n",
    "    def _best_candidate(self, prev):\n",
    "        candidates = ((ngram[-1],prob) for ngram,prob in self.model.items() if ngram[:-1]==prev)\n",
    "        candidates = sorted(candidates, key=lambda candidate: candidate[1], reverse=True)\n",
    "        if len(candidates) == 0:\n",
    "            return \"</s>\", 1\n",
    "        else:\n",
    "            return candidates[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T16:37:47.593961Z",
     "start_time": "2024-03-20T16:37:47.552940Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "n = 5\n",
    "lm5 = LanguageModel(train, n)\n",
    "lm4 = LanguageModel(train, n-1)\n",
    "lm3 = LanguageModel(train, n-2)\n",
    "lm2 = LanguageModel(train, n-3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T16:40:23.146963Z",
     "start_time": "2024-03-20T16:37:47.593388Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def split_and_give_predictions(tokens, lm, n):\n",
    "    best_candidates = []\n",
    "\n",
    "    for i in range(len(tokens) - (n - 1)):\n",
    "        context = tuple(tokens[i:i+(n - 1)])\n",
    "    \n",
    "        # Get the best candidate for the current context\n",
    "        candidate, probability = lm._best_candidate(context)\n",
    "\n",
    "        print(context, candidate, probability)\n",
    "    \n",
    "        # Add the best candidate to the list\n",
    "        best_candidates.append((candidate, probability))\n",
    "    \n",
    "    # Print the best candidates\n",
    "    for candidate, probability in best_candidates:\n",
    "        print(f\"Best candidate: {candidate}, Probability: {probability}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T16:44:48.901Z",
     "start_time": "2024-03-20T16:44:48.800067Z"
    }
   },
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best candidate: choice, Probability: 2.4359942510535674e-05\n"
     ]
    }
   ],
   "source": [
    "splitted_text = \"Fox was the perfect\".split()\n",
    "\n",
    "# Get the best candidate for the current context\n",
    "def stupid_backoff(context, models):\n",
    "    for i in range(1, len(models) + 1):\n",
    "        lm = models[i - 1]\n",
    "        if i != 1:\n",
    "            context = context[1:]\n",
    "        candidate, probability = lm._best_candidate(tuple(context))\n",
    "        if candidate != \"</s>\":\n",
    "            return candidate, probability\n",
    "    return \"</s>\", 1\n",
    "\n",
    "models = [lm5, lm4, lm3, lm2]\n",
    "best_candidate, best_probability = stupid_backoff(splitted_text, models)\n",
    "print(f\"Best candidate: {best_candidate}, Probability: {best_probability}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T16:44:57.957921Z",
     "start_time": "2024-03-20T16:44:48.891205Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_test_data(test_path, percent=1):\n",
    "    with open(test_path, 'r') as f:\n",
    "        test = [l.strip() for l in f.readlines()]\n",
    "    total_samples = len(test)\n",
    "    num_samples_to_load = int(total_samples * percent / 100)\n",
    "    return test[-num_samples_to_load:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T16:44:57.967885Z",
     "start_time": "2024-03-20T16:44:57.784325Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test = load_test_data(\"/Users/damirabdulaev/Downloads/aclImdb/train/combined.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T16:44:59.007612Z",
     "start_time": "2024-03-20T16:44:57.795880Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def introduce_mistakes(test_data, probability, n):\n",
    "    # Iterate over each sentence in the test data\n",
    "    sentences = []\n",
    "    for sentence in test_data:\n",
    "        # Split the sentence into words\n",
    "        words = sentence.split()\n",
    "\n",
    "        # Determine the number of words to introduce mistakes into\n",
    "        num_words_to_modify = max(1, round(len(words) * probability))\n",
    "        \n",
    "        # Randomly select words to modify\n",
    "        if len(words) == 0:\n",
    "            continue\n",
    "        words_to_modify = random.sample(words, num_words_to_modify)\n",
    "\n",
    "        # Introduce mistakes into selected words\n",
    "        for word in words_to_modify:\n",
    "            # Randomly select positions to modify within the word\n",
    "            positions_to_modify = random.sample(range(len(word)), min(n, len(word)))\n",
    "\n",
    "            # Introduce mistakes at selected positions\n",
    "            modified_word = list(word)\n",
    "            for pos in positions_to_modify:\n",
    "                # Introduce a random misspelled letter at the selected position\n",
    "                modified_word[pos] = random.choice([char for char in 'abcdefghijklmnopqrstuvwxyz' if char != word[pos].lower()])\n",
    "            modified_word = ''.join(modified_word)\n",
    "\n",
    "            # Replace the original word with the modified word in the sentence\n",
    "            sentence = sentence.replace(word, modified_word)\n",
    "            \n",
    "            sentences.append(sentence)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "mistaken_sentences = introduce_mistakes(test, probability=0.01, n=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T16:44:59.147447Z",
     "start_time": "2024-03-20T16:44:59.094710Z"
    }
   },
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'horror flick fall backs gore pointless nudity kkocvs against the catholic church'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistaken_sentences[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T16:44:59.173972Z",
     "start_time": "2024-03-20T16:44:59.169214Z"
    }
   },
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thsi', 'is', 'a']\n",
      "['Thsi', 'is', 'a', '</s>']\n",
      "Original: Thsi is a tst sentence.\n",
      "Corrected: Thsi is a </s> \n",
      "\n",
      "[]\n",
      "['</s>', 'example']\n",
      "Original: Anohter example sentence.\n",
      "Corrected: </s> example </s>\n",
      "\n",
      "['Yet']\n",
      "['Yet', '</s>']\n",
      "Original: Yet antoher example.\n",
      "Corrected: Yet </s> </s>\n"
     ]
    }
   ],
   "source": [
    "def replace_with_best_candidate(sentence, models):\n",
    "    # Split the sentence into words\n",
    "    words = sentence.split()\n",
    "\n",
    "    # Iterate over each word in the sentence\n",
    "    for i, word in enumerate(words):\n",
    "        # Check if the word is not in the vocabulary\n",
    "        if word not in models[0].vocab:\n",
    "            # Create the context for the current word\n",
    "            \n",
    "            context = words[:i]\n",
    "            print(context)\n",
    "\n",
    "            # Get the best candidate for the current context\n",
    "            best_candidate, _ = stupid_backoff(context, models)\n",
    "\n",
    "            # Replace the original word with the best candidate\n",
    "            words[i] = best_candidate\n",
    "\n",
    "    # Join the words back into a sentence\n",
    "    corrected_sentence = ' '.join(words)\n",
    "    return corrected_sentence\n",
    "\n",
    "# Example usage\n",
    "test_sentences = [\n",
    "    \"Thsi is a tst sentence.\",\n",
    "    \"Anohter example sentence.\",\n",
    "    \"Yet antoher example.\"\n",
    "]\n",
    "\n",
    "# Iterate over each test sentence and replace mistaken words with best candidates\n",
    "for sentence in test_sentences:\n",
    "    corrected_sentence = replace_with_best_candidate(sentence, models)\n",
    "    print(\"Original:\", sentence)\n",
    "    print(\"Corrected:\", corrected_sentence)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T17:09:25.849623Z",
     "start_time": "2024-03-20T17:07:59.113073Z"
    }
   },
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oML-5sJwGRLE"
   },
   "source": [
    "## Justify your decisions\n",
    "\n",
    "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
    "- Which ngram dataset to use\n",
    "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
    "- Beam search parameters\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Xb_twOmVsC6"
   },
   "source": [
    "*Your text here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46rk65S4GRSe"
   },
   "source": [
    "## Evaluate on a test set\n",
    "\n",
    "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwZWaX9VVs7B"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
